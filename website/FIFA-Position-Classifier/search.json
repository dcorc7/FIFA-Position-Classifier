[
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "Results",
    "section": "",
    "text": "The neural network model for classifying players into their generic football positions—attacker, midfielder, and defender—demonstrated strong performance across all evaluation metrics. The model architecture and training procedure were carefully optimized using a grid search over various hyperparameters. The best configuration consisted of three hidden layers, each with 256 neurons, a dropout rate of 0.4, and the Adam optimizer with a learning rate of 0.0001. This configuration yielded a best validation loss of 0.6764, as determined over 39 training epochs.\nDuring training, the model’s performance steadily improved, with the validation loss decreasing from 0.6933 in epoch 1 to 0.6722 by epoch 34, and achieving a final test loss of 0.6715. This suggests that the model not only learned meaningful representations but also generalized well to unseen data.\n\n\n\nTraining and validation loss curves for the generic position model\n\n\nThe model’s classification performance was evaluated using accuracy, precision, recall, and F1 score, which were derived from the multi-class confusion matrix. The overall accuracy of the model was 0.8776, indicating that nearly 88% of the predictions matched the true generic position labels. The precision (0.8772) and recall (0.8776) scores reflect a high degree of correctness and completeness in the model’s predictions, respectively. The F1 score of 0.8766 further confirms the balanced nature of the model’s performance.\nThe following table presents the performance metrics for the generic classification task:\n\n\n\nClass\nPrecision\nRecall\nF1 Score\nSupport\n\n\n\n\nAttackers\n0.8440\n0.7328\n0.7845\n1048\n\n\nDefenders\n0.9316\n0.9456\n0.9385\n2406\n\n\nMidfielders\n0.8365\n0.8727\n0.8542\n2357\n\n\n\n\nAttackers (Class 0):\nThe model shows good precision (0.8440) but struggles with recall (0.7328), resulting in a moderate F1 score of 0.7845. This indicates that while the model is accurate when predicting attackers, it fails to capture some true attackers, leading to false negatives.\nDefenders (Class 1):\nDefenders are the best predicted class, with a high precision (0.9316) and recall (0.9456), resulting in an outstanding F1 score of 0.9385. The model is both accurate and comprehensive in identifying defenders, showing strong performance in this category.\nMidfielders (Class 2):\nThe model maintains solid performance for midfielders with a precision of 0.8365 and recall of 0.8727, yielding a strong F1 score of 0.8542. This indicates a balanced approach to predicting midfielders, with relatively few false positives or negatives.\n\n\n\n\nConfusion Matrix for the generic position model\n\n\nThese class-level metrics confirm that the model performs consistently across all three generic positions, though improvements could be made in enhancing recall for attackers. The macro-averaged F1 score of 0.8591 and weighted average F1 score of 0.8766 further validate the model’s robustness across the imbalanced class distribution.\nWhile the current results demonstrate a high-performing model, further enhancements may be possible through the exploration of more complex architectures (e.g., attention mechanisms or residual connections) or additional input features that capture contextual positional data. Nonetheless, this neural network serves as a strong baseline for generic position classification tasks in football analytics."
  },
  {
    "objectID": "results.html#neural-network-generic-position-classification",
    "href": "results.html#neural-network-generic-position-classification",
    "title": "Results",
    "section": "",
    "text": "The neural network model for classifying players into their generic football positions—attacker, midfielder, and defender—demonstrated strong performance across all evaluation metrics. The model architecture and training procedure were carefully optimized using a grid search over various hyperparameters. The best configuration consisted of three hidden layers, each with 256 neurons, a dropout rate of 0.4, and the Adam optimizer with a learning rate of 0.0001. This configuration yielded a best validation loss of 0.6764, as determined over 39 training epochs.\nDuring training, the model’s performance steadily improved, with the validation loss decreasing from 0.6933 in epoch 1 to 0.6722 by epoch 34, and achieving a final test loss of 0.6715. This suggests that the model not only learned meaningful representations but also generalized well to unseen data.\n\n\n\nTraining and validation loss curves for the generic position model\n\n\nThe model’s classification performance was evaluated using accuracy, precision, recall, and F1 score, which were derived from the multi-class confusion matrix. The overall accuracy of the model was 0.8776, indicating that nearly 88% of the predictions matched the true generic position labels. The precision (0.8772) and recall (0.8776) scores reflect a high degree of correctness and completeness in the model’s predictions, respectively. The F1 score of 0.8766 further confirms the balanced nature of the model’s performance.\nThe following table presents the performance metrics for the generic classification task:\n\n\n\nClass\nPrecision\nRecall\nF1 Score\nSupport\n\n\n\n\nAttackers\n0.8440\n0.7328\n0.7845\n1048\n\n\nDefenders\n0.9316\n0.9456\n0.9385\n2406\n\n\nMidfielders\n0.8365\n0.8727\n0.8542\n2357\n\n\n\n\nAttackers (Class 0):\nThe model shows good precision (0.8440) but struggles with recall (0.7328), resulting in a moderate F1 score of 0.7845. This indicates that while the model is accurate when predicting attackers, it fails to capture some true attackers, leading to false negatives.\nDefenders (Class 1):\nDefenders are the best predicted class, with a high precision (0.9316) and recall (0.9456), resulting in an outstanding F1 score of 0.9385. The model is both accurate and comprehensive in identifying defenders, showing strong performance in this category.\nMidfielders (Class 2):\nThe model maintains solid performance for midfielders with a precision of 0.8365 and recall of 0.8727, yielding a strong F1 score of 0.8542. This indicates a balanced approach to predicting midfielders, with relatively few false positives or negatives.\n\n\n\n\nConfusion Matrix for the generic position model\n\n\nThese class-level metrics confirm that the model performs consistently across all three generic positions, though improvements could be made in enhancing recall for attackers. The macro-averaged F1 score of 0.8591 and weighted average F1 score of 0.8766 further validate the model’s robustness across the imbalanced class distribution.\nWhile the current results demonstrate a high-performing model, further enhancements may be possible through the exploration of more complex architectures (e.g., attention mechanisms or residual connections) or additional input features that capture contextual positional data. Nonetheless, this neural network serves as a strong baseline for generic position classification tasks in football analytics."
  },
  {
    "objectID": "results.html#neural-network-specific-position-classification",
    "href": "results.html#neural-network-specific-position-classification",
    "title": "Results",
    "section": "Neural Network – Specific Position Classification",
    "text": "Neural Network – Specific Position Classification\nThe neural network model developed to classify players into their specific football positions—such as central midfielders, wingers, fullbacks, and strikers—used a more complex architecture and optimization process compared to the generic model. A comprehensive grid search was conducted across multiple hyperparameter combinations, ultimately selecting the following best configuration: four hidden layers with 128 neurons each, a dropout rate of 0.4, the RMSprop optimizer, and a learning rate of 0.0001. This setup achieved a best validation loss of 1.7440 after training for 60 epochs.\nThe model exhibited gradual improvement during training, with the validation loss decreasing from 1.8684 in epoch 1 to 1.7367 by epoch 55, before reaching a final test loss of 1.7404. This consistent convergence suggests effective learning, although the loss plateaued in the latter epochs, hinting at the limits of this architecture for such a fine-grained task.\n\n\n\nTraining and validation loss curves for the specific position model\n\n\nDespite successful convergence, the overall classification performance was moderate due to the increased complexity of distinguishing between nine specific player positions. The model achieved an accuracy of 0.6262, with a precision of 0.5640, recall of 0.6262, and F1 score of 0.5920. These metrics indicate that while the model can often identify the correct position, it struggles with class-specific consistency—particularly for underrepresented classes.\nThe following table summarizes the performance metrics for each class in the specific position classification task:\n\n\n\nClass\nPrecision\nRecall\nF1 Score\nSupport\n\n\n\n\n0.0\n0.8927\n0.8955\n0.8941\n1254\n\n\n1.0\n0.0000\n0.0000\n0.0000\n474\n\n\n2.0\n0.7932\n0.7992\n0.7962\n528\n\n\n3.0\n0.3514\n0.4747\n0.4039\n969\n\n\n4.0\n0.0000\n0.0000\n0.0000\n145\n\n\n5.0\n0.7204\n0.8096\n0.7624\n541\n\n\n6.0\n0.4150\n0.5041\n0.4553\n974\n\n\n7.0\n0.0000\n0.0000\n0.0000\n136\n\n\n8.0\n0.7655\n0.8924\n0.8241\n790\n\n\n\n\nCenter Backs (Class 0):\nThe model performed exceptionally well for center-backs with high precision (0.8927) and recall (0.8955), resulting in an F1 score of 0.8941. This suggests that the model reliably identifies this position with few false positives and false negatives.\nCenter Midfielders (Class 1):\nThe model failed to classify center midfielders, resulting in a precision and recall of 0.0000. This could be due to the underrepresentation of left-backs in the training data or insufficient discriminative features in the input data for this role.\nLeft Backs (Class 2):\nLeft-backs were classified well with precision (0.7932) and recall (0.7992), yielding a strong F1 score of 0.7962. This indicates that the model can reliably classify these players.\nLeft Midfielders (Class 3):\nPerformance for Left midfielders was moderate, with a F1 score of 0.4039. The model exhibited weaker results in both precision and recall, which may be due to the complex nature of this position, which often overlaps with other roles.\nLeft Wingers (Class 4):\nLeft wingers were poorly classified with precision and recall both at 0.0000. The lack of success here points to a data imbalance or feature overlap causing difficulty in learning the distinguishing traits of defensive midfielders.\nRight Backs (Class 5):\nThe model performed well with right backs, scoring precision (0.7204), recall (0.8096), and an F1 score of 0.7624, indicating strong classification results for this position.\nRight Midfielders (Class 6):\nThe model’s F1 score for right midfielders was 0.4553, with moderate performance in both precision and recall. Improving precision would help reduce false positives for this position.\nRight Wingers (Class 7):\nSimilar to other underrepresented roles, right wingers had a precision and recall of 0.0000. This suggests the model was unable to effectively identify this position, likely due to insufficient data or feature ambiguity.\nStrikers (Class 8):\nThe model performed strongly with strikers, achieving an F1 score of 0.8241. Both precision (0.7655) and recall (0.8924) were high, showing that the model accurately identified this attacking role.\n\n\n\n\nConfusion Matrix for the specific position model\n\n\nOverall, while the model demonstrated strong performance for certain positions, such as center backs (Class 0) and strikers (Class 8), there is a significant imbalance in how it classifies less-represented positions, like left midfielders (Class 3) and right wingers (Class 7). These results highlight areas for potential improvement, such as data augmentation, class balancing, or incorporating positional embeddings into the model.\nThe macro-averaged F1 score of 0.4595 reflects the variance in performance across classes, while the weighted average F1 score of 0.5920 shows that the model performs better on the more prevalent classes. These results indicate that while the model captures positional patterns for well-represented roles, like center backs and strikers, it struggles with niche or overlapping classes such as center midfielders (Class 1) and left wingers (Class 4).\nFuture improvements could involve class balancing techniques, positional embeddings, or sequence-aware models to capture tactical roles more dynamically. Nonetheless, the current model offers a useful first step in fine-grained player role prediction."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Classifying Soccer Player Pistions Using FIFA Card Statistics",
    "section": "",
    "text": "Introduction\nUnderstanding player roles is fundamental in modern soccer analytics, as they influence scouting decisions, tactical planning, and team building. While traditional positional labels like “defender” or “midfielder” provide a general overview of a player’s role, more granular classifications such as “Striker (ST)” or “Right Back (RB)” reveal critical tactical nuances. This project aims to classify soccer players into their on-field roles using neural networks trained on player attributes from EA Sports FIFA video game data (FIFA 15–23). Two multi-class classification models were developed: one that assigns players to broad position groups (attacker, midfielder, or defender) and another that classifies them into specific roles, including ST, LW, CM, CB, and others.\n\n\nLiterature Review\nThe Github repository hosting all code for this project can be found here\n\n\n\n\n Back to top"
  },
  {
    "objectID": "data_collection.html",
    "href": "data_collection.html",
    "title": "Data & Feature Engineering",
    "section": "",
    "text": "Dataset:\nThe dataset used in this project was sourced from the publicly available FIFA 23 Complete Player Dataset on Kaggle, created by Stefano Leone. This comprehensive dataset spans multiple editions of the FIFA video game series, from FIFA 15 through FIFA 23, and includes detailed information on thousands of players from each game year. Upon initial download, the dataset comprised 161,583 rows and 110 columns, representing a wide array of player data across seasons.\nThe raw data contains a diverse range of features, including player biographical information (e.g., name, nationality, age, height, weight), game-specific ratings (e.g., overall score, potential), position labels, in-game attributes (e.g., passing accuracy, dribbling, strength, acceleration), and categorical characteristics such as preferred foot, work rate, and body type. Additionally, the dataset includes financial estimations like player value and wage in euros, which help contextualize a player’s market standing within the game. Some columns also contain metadata, such as unique player IDs, contract details, and club affiliation.\nThis rich and multifaceted dataset provides the foundation for training neural networks capable of positional classification. However, before modeling, a thorough data cleaning and preprocessing pipeline was necessary to reduce noise, handle missing or irrelevant entries, and retain only the features relevant to our classification tasks.\nThe raw dataset can be downloaded here\n\n\nPreprocessing:\nTo prepare the dataset for modeling, a data preprocessing pipeline was implemented to clean and transform the raw FIFA data into an appropriate format for machine learning. Initially, the dataset consisted of 161,583 rows and 110 columns, including a wide mix of metadata, gameplay attributes, and positional information. However, many of these columns were either redundant, unrelated to the classification task, or could potentially leak information about the target labels. For instance, columns like player_positions, nation_position, and various specific in-game ratings for each position (ST, CB, CAM, etc.) explicitly revealed the player’s role and were therefore dropped to maintain the integrity of the classification task. Additional columns related to player identification, image URLs, and contract metadata were also removed, significantly reducing the dimensionality and potential noise in the data.\nAfter dropping irrelevant columns, rows containing missing values were removed to ensure a complete dataset, resulting in a cleaner and more consistent input frame. Categorical string values such as specific_position, preferred_foot, work_rate, and body_type were standardized and explicitly cast to string types. Special care was taken to filter out players marked as “SUB” (substitute) or “RES” (reserve), since these entries do not reflect a clear on-field role.\nTo facilitate modeling, player positions were mapped into a controlled set of categories. The specific_position column, which originally contained over 25 detailed FIFA position labels, was consolidated into nine standardized positions: ST, LW, RW, LM, RM, CM, CB, LB, and RB. These were then further grouped into the three broader positional categories of attacker, midfielder, and defender under a new generic_position column. In addition, body types were normalized into four groups: Lean, Normal, Stocky, and Unique, simplifying what was originally a more granular and inconsistent set of labels.\nCategorical variables were one-hot encoded to convert them into a numeric format compatible with neural network input. This included preferred_foot, work_rate, and body_type. The specific and generic position labels were also encoded using label encoders to create integer class labels for classification.\nFinally, all numeric features, such as player attributes like pace, shooting, strength, and value, were standardized using StandardScaler to ensure that they shared a common scale. This step is especially important for neural networks, which are sensitive to the magnitude of input values. Boolean fields were converted to binary integers to ensure compatibility during model training. By the end of this pipeline, the dataset was fully numeric, clean, and appropriately structured for training multi-class classification models.\n\n\nFinal Input Format:\nThe final dataset used for training the neural network models consists of 57 input features, capturing both player characteristics and in-game performance statistics. These include biometric data, technical and physical attributes, financial metrics, and one-hot encoded categorical variables such as preferred foot, work rate, and body type. The dataset also includes encoded versions of the specific and generic positional labels, which serve as the target variables for the multi-class classification tasks.\nThe final features and a short description of each can be found below:\n\n\n\n\n\n\n\n\nIndex\nFeature Name\nDescription\n\n\n\n\n1\noverall\nOverall rating representing player quality\n\n\n2\nvalue_eur\nEstimated market value in euros\n\n\n3\nwage_eur\nWeekly wage in euros\n\n\n4\nage\nPlayer’s age in years\n\n\n5\nheight_cm\nPlayer’s height in centimeters\n\n\n6\nweight_kg\nPlayer’s weight in kilograms\n\n\n7\nweak_foot\nRating (1–5) of the player’s ability with non-dominant foot\n\n\n8\nskill_moves\nRating (1–5) of the player’s special skill move ability\n\n\n9\npace\nOverall pace rating (acceleration + sprint speed)\n\n\n10\nshooting\nOverall shooting rating\n\n\n11\npassing\nOverall passing rating\n\n\n12\ndribbling\nOverall dribbling rating\n\n\n13\ndefending\nOverall defending rating\n\n\n14\nphysic\nOverall physical ability rating\n\n\n15\nattacking_crossing\nAbility to cross the ball\n\n\n16\nattacking_finishing\nAbility to finish scoring chances\n\n\n17\nattacking_heading_accuracy\nHeading accuracy during attacks\n\n\n18\nattacking_short_passing\nAccuracy and effectiveness of short passes\n\n\n19\nattacking_volleys\nAccuracy and technique when volleying\n\n\n20\nskill_dribbling\nTechnical dribbling ability\n\n\n21\nskill_curve\nAbility to curve the ball\n\n\n22\nskill_fk_accuracy\nAccuracy when taking free kicks\n\n\n23\nskill_long_passing\nAccuracy and effectiveness of long passes\n\n\n24\nskill_ball_control\nBall control under pressure\n\n\n25\nmovement_acceleration\nQuickness from a standstill\n\n\n26\nmovement_sprint_speed\nTop sprint speed\n\n\n27\nmovement_agility\nAbility to turn and change direction quickly\n\n\n28\nmovement_reactions\nSpeed of reaction to gameplay situations\n\n\n29\nmovement_balance\nAbility to stay balanced under pressure\n\n\n30\npower_shot_power\nPower behind shots\n\n\n31\npower_jumping\nJumping ability\n\n\n32\npower_stamina\nAbility to sustain high performance over time\n\n\n33\npower_strength\nPhysical strength in duels\n\n\n34\npower_long_shots\nAccuracy and power of long-distance shots\n\n\n35\nmentality_aggression\nAggression and intensity in play\n\n\n36\nmentality_interceptions\nAbility to read the game and intercept passes\n\n\n37\nmentality_positioning\nAttacking positioning without the ball\n\n\n38\nmentality_vision\nAbility to see passing opportunities\n\n\n39\nmentality_penalties\nSkill in taking penalty kicks\n\n\n40\ndefending_marking_awareness\nAbility to mark and track players defensively\n\n\n41\ndefending_standing_tackle\nEffectiveness of standing tackles\n\n\n42\ndefending_sliding_tackle\nEffectiveness of sliding tackles\n\n\n43\npreferred_foot_Left\nBinary indicator: player prefers left foot\n\n\n44\npreferred_foot_Right\nBinary indicator: player prefers right foot\n\n\n45\nwork_rate_High/High\nBinary indicator for high attacking and high defensive work rate\n\n\n46\nwork_rate_High/Low\nBinary indicator for high attacking and low defensive work rate\n\n\n47\nwork_rate_High/Medium\nBinary indicator for high attacking and medium defensive work rate\n\n\n48\nwork_rate_Low/High\nBinary indicator for low attacking and high defensive work rate\n\n\n49\nwork_rate_Low/Low\nBinary indicator for low attacking and low defensive work rate\n\n\n50\nwork_rate_Low/Medium\nBinary indicator for low attacking and medium defensive work rate\n\n\n51\nwork_rate_Medium/High\nBinary indicator for medium attacking and high defensive work rate\n\n\n52\nwork_rate_Medium/Low\nBinary indicator for medium attacking and low defensive work rate\n\n\n53\nwork_rate_Medium/Medium\nBinary indicator for medium attacking and medium defensive work rate\n\n\n54\nbody_type_Lean\nBinary indicator: lean body type\n\n\n55\nbody_type_Normal\nBinary indicator: normal body type\n\n\n56\nbody_type_Stocky\nBinary indicator: stocky body type\n\n\n57\nbody_type_Unique\nBinary indicator: unique body type (e.g., custom model)\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "methods.html",
    "href": "methods.html",
    "title": "Methods",
    "section": "",
    "text": "This section outlines the methodological framework used to classify FIFA 23 players into position categories using neural networks. The process involved designing and training multi-layer feedforward neural networks on a curated and preprocessed dataset of player attributes. The aim was to build models capable of learning complex patterns in the data and accurately predicting either the specific or generic on-pitch positions of football players. The methodology emphasizes model design, training strategies, and optimization procedures to ensure generalizability and performance."
  },
  {
    "objectID": "methods.html#multi-class-classification",
    "href": "methods.html#multi-class-classification",
    "title": "Methods",
    "section": "Multi-Class Classification",
    "text": "Multi-Class Classification\nTo classify players into position categories, two separate neural network models were developed:\n\nOne for generic positions (Attacker, Midfielder, Defender) with an output layer of 3 neurons\nOne for specific positions (CB, LB, RB, CM, LM, RM, ST, LW, RW) with an output layer of 9 neurons\n\nBoth models used a final dense (fully connected) layer followed by a softmax activation function. The softmax function converts the raw output logits into class probabilities that sum to one, enabling a probabilistic interpretation of the model’s predictions.\nThe networks were trained using the categorical cross-entropy (CCE) loss function, which is suitable for multi-class classification tasks. Predictions were obtained by taking the argmax of the output probability vector—selecting the class with the highest predicted probability.\nThroughout training, the choice of optimizer (Adam, SGD, or RMSProp) was based on performance observed during a manual hyperparameter tuning process. Model evaluation focused on both accuracy and generalization, assessed through performance on a held-out validation set."
  }
]